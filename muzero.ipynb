{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the muzero psuedocode, following the approach and cod from https://medium.com/applied-data-science/how-to-build-your-own-muzero-in-python-f77d5718061a.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lint as: python3\n",
    "\"\"\"Pseudocode description of the MuZero algorithm.\"\"\"\n",
    "# pylint: disable=unused-argument\n",
    "# pylint: disable=missing-docstring\n",
    "# pylint: disable=g-explicit-length-test\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "# from __future__ import google_type_annotations\n",
    "from __future__ import print_function\n",
    "\n",
    "import importlib\n",
    "import gym\n",
    "import collections\n",
    "import math\n",
    "import typing\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "class MuZero:\n",
    "    \"\"\"\n",
    "    Main class to manage MuZero.\n",
    "\n",
    "    Args:\n",
    "        game_name (str): Name of the game module, it should match the name of a .py file in the \"./games\" directory.\n",
    "\n",
    "    Example:\n",
    "        >>> muzero = MuZero(\"cartpole\")\n",
    "        >>> muzero.train()\n",
    "        >>> muzero.test()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, game_name):\n",
    "        self.game_name = game_name\n",
    "\n",
    "        # Load the game and the config from the module with the game name\n",
    "        try:\n",
    "            game_module = importlib.import_module(\"games.\" + self.game_name)\n",
    "#             self.config = game_module.MuZeroConfig()\n",
    "            self.Game = game_module.Game\n",
    "        except Exception as err:\n",
    "            print(\n",
    "                '{} is not a supported game name, try \"cartpole\" or refer to the documentation for adding a new game.'.format(\n",
    "                    self.game_name\n",
    "                )\n",
    "            )\n",
    "            raise err\n",
    "\n",
    "\n",
    "##########################\n",
    "####### Helpers ##########\n",
    "\n",
    "MAXIMUM_FLOAT_VALUE = float('inf')\n",
    "\n",
    "KnownBounds = collections.namedtuple('KnownBounds', ['min', 'max'])\n",
    "\n",
    "\n",
    "class MinMaxStats(object):\n",
    "    \"\"\"A class that holds the min-max values of the tree.\"\"\"\n",
    "\n",
    "    def __init__(self, known_bounds: Optional[KnownBounds]):\n",
    "        self.maximum = known_bounds.max if known_bounds else -MAXIMUM_FLOAT_VALUE\n",
    "        self.minimum = known_bounds.min if known_bounds else MAXIMUM_FLOAT_VALUE\n",
    "\n",
    "    def update(self, value: float):\n",
    "        self.maximum = max(self.maximum, value)\n",
    "        self.minimum = min(self.minimum, value)\n",
    "\n",
    "    def normalize(self, value: float) -> float:\n",
    "        if self.maximum > self.minimum:\n",
    "        # We normalize only when we have set the maximum and minimum values.\n",
    "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
    "        return value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MuZero at 0x649f955748>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MuZero(\"TicTacToe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuZeroConfig(object):\n",
    "\n",
    "    def __init__(self,\n",
    "               action_space_size: int,\n",
    "               max_moves: int,\n",
    "               discount: float,\n",
    "               dirichlet_alpha: float,\n",
    "               num_simulations: int,\n",
    "               batch_size: int,\n",
    "               td_steps: int,\n",
    "               num_actors: int,\n",
    "               lr_init: float,\n",
    "               lr_decay_steps: float,\n",
    "               visit_softmax_temperature_fn,\n",
    "               known_bounds: Optional[KnownBounds] = None,\n",
    "                #New\n",
    "               game_name = \"TicTacToe\"):\n",
    "    ### Self-Play\n",
    "        self.action_space_size = action_space_size\n",
    "        self.num_actors = num_actors\n",
    "\n",
    "        self.visit_softmax_temperature_fn = visit_softmax_temperature_fn\n",
    "        self.max_moves = max_moves\n",
    "        self.num_simulations = num_simulations\n",
    "        self.discount = discount\n",
    "\n",
    "        # Root prior exploration noise.\n",
    "        self.root_dirichlet_alpha = dirichlet_alpha\n",
    "        self.root_exploration_fraction = 0.25\n",
    "\n",
    "        # UCB formula\n",
    "        self.pb_c_base = 19652\n",
    "        self.pb_c_init = 1.25\n",
    "\n",
    "        # If we already have some information about which values occur in the\n",
    "        # environment, we can use them to initialize the rescaling.\n",
    "        # This is not strictly necessary, but establishes identical behaviour to\n",
    "        # AlphaZero in board games.\n",
    "        self.known_bounds = known_bounds\n",
    "\n",
    "        ### Training\n",
    "        self.training_steps = int(1000e3)\n",
    "        self.checkpoint_interval = int(1e3)\n",
    "        self.window_size = int(1e6)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_unroll_steps = 5\n",
    "        self.td_steps = td_steps\n",
    "\n",
    "        self.weight_decay = 1e-4\n",
    "        self.momentum = 0.9\n",
    "\n",
    "        # Exponential learning rate schedule\n",
    "        self.lr_init = lr_init\n",
    "        self.lr_decay_rate = 0.1\n",
    "        self.lr_decay_steps = lr_decay_steps\n",
    "        \n",
    "        self.game_name = game_name\n",
    "        \n",
    "        try:\n",
    "            game_module = importlib.import_module(\"games.\" + self.game_name)\n",
    "#             self.config = game_module.MuZeroConfig()\n",
    "            self.Game = game_module.Game\n",
    "        except Exception as err:\n",
    "            print(\n",
    "                '{} is not a supported game name, try \"cartpole\" or refer to the documentation for adding a new game.'.format(\n",
    "                    self.game_name\n",
    "                )\n",
    "            )\n",
    "            raise err\n",
    "\n",
    "\n",
    "    def new_game(self):\n",
    "        return self.Game()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_board_game_config(action_space_size: int, max_moves: int,\n",
    "                           dirichlet_alpha: float,\n",
    "                           lr_init: float) -> MuZeroConfig:\n",
    "\n",
    "    def visit_softmax_temperature(num_moves, training_steps):\n",
    "        if num_moves < 30:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0  # Play according to the max.\n",
    "\n",
    "    return MuZeroConfig(\n",
    "        action_space_size=action_space_size,\n",
    "        max_moves=max_moves,\n",
    "        discount=1.0,\n",
    "        dirichlet_alpha=dirichlet_alpha,\n",
    "        num_simulations=800,\n",
    "        batch_size=2048,\n",
    "        td_steps=max_moves,  # Always use Monte Carlo return.\n",
    "        num_actors=3000,\n",
    "        lr_init=lr_init,\n",
    "        lr_decay_steps=400e3,\n",
    "        visit_softmax_temperature_fn=visit_softmax_temperature,\n",
    "        known_bounds=KnownBounds(-1, 1))\n",
    "\n",
    "\n",
    "def make_go_config() -> MuZeroConfig:\n",
    "    return make_board_game_config(\n",
    "        action_space_size=362, max_moves=722, dirichlet_alpha=0.03, lr_init=0.01)\n",
    "\n",
    "\n",
    "def make_chess_config() -> MuZeroConfig:\n",
    "    return make_board_game_config(\n",
    "        action_space_size=4672, max_moves=512, dirichlet_alpha=0.3, lr_init=0.1)\n",
    "\n",
    "\n",
    "def make_shogi_config() -> MuZeroConfig:\n",
    "    return make_board_game_config(\n",
    "        action_space_size=11259, max_moves=512, dirichlet_alpha=0.15, lr_init=0.1)\n",
    "\n",
    "#New\n",
    "def make_tictactoe_config() -> MuZeroConfig:\n",
    "    return make_board_game_config(\n",
    "        action_space_size=9, max_moves=9, dirichlet_alpha=0.1, lr_init=0.1)\n",
    "#End\n",
    "\n",
    "def make_atari_config() -> MuZeroConfig:\n",
    "\n",
    "    def visit_softmax_temperature(num_moves, training_steps):\n",
    "        if training_steps < 500e3:\n",
    "            return 1.0\n",
    "        elif training_steps < 750e3:\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.25\n",
    "\n",
    "    return MuZeroConfig(\n",
    "          action_space_size=18,\n",
    "          max_moves=27000,  # Half an hour at action repeat 4.\n",
    "          discount=0.997,\n",
    "          dirichlet_alpha=0.25,\n",
    "          num_simulations=50,\n",
    "          batch_size=1024,\n",
    "          td_steps=10,\n",
    "          num_actors=350,\n",
    "          lr_init=0.05,\n",
    "          lr_decay_steps=350e3,\n",
    "          visit_softmax_temperature_fn=visit_softmax_temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = make_tictactoe_config()\n",
    "#dir(my_config)\n",
    "#print(my_config.__class__)\n",
    "#MuZeroConfig.print_garbage(MuZeroConfig)\n",
    "#dir(my_config)\n",
    "#my_config.print_garbage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(object):\n",
    "\n",
    "    def __init__(self, index: int):\n",
    "        self.index = index\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.index\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.index == other.index\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.index > other.index\n",
    "\n",
    "\n",
    "class Player(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "\n",
    "    def __init__(self, prior: float):\n",
    "        self.visit_count = 0\n",
    "        self.to_play = -1\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_state = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def expanded(self) -> bool:\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self) -> float:\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ActionHistory(object):\n",
    "    \"\"\"Simple history container used inside the search.\n",
    "\n",
    "    Only used to keep track of the actions executed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, history: List[Action], action_space_size: int):\n",
    "        self.history = list(history)\n",
    "        self.action_space_size = action_space_size\n",
    "\n",
    "    def clone(self):\n",
    "        return ActionHistory(self.history, self.action_space_size)\n",
    "\n",
    "    def add_action(self, action: Action):\n",
    "        self.history.append(action)\n",
    "\n",
    "    def last_action(self) -> Action:\n",
    "        return self.history[-1]\n",
    "\n",
    "    def action_space(self) -> List[Action]:\n",
    "        return [Action(i) for i in range(self.action_space_size)]\n",
    "\n",
    "    def to_play(self) -> Player:\n",
    "        return Player()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Environment(object):\n",
    "    \"\"\"The environment MuZero is interacting with.\"\"\"\n",
    "\n",
    "    def step(self, action):\n",
    "        pass\n",
    "\n",
    "\n",
    "class GameHistory(object):\n",
    "    \"\"\"A single episode of interaction with the environment.\"\"\"\n",
    "    \"\"\"This is a game history.\"\"\"\n",
    "\n",
    "    def __init__(self, action_space_size: int, discount: float):\n",
    "        #Old\n",
    "        #self.environment = Environment()  # Game specific environment.\n",
    "        #New\n",
    "        #self.environment = game_module.Game\n",
    "        #End\n",
    "        self.history = []\n",
    "        self.rewards = []\n",
    "        self.child_visits = []\n",
    "        self.root_values = []\n",
    "        self.action_space_size = action_space_size\n",
    "        self.discount = discount\n",
    "\n",
    "    def terminal(self) -> bool:\n",
    "        # Game specific termination rules.\n",
    "        #New\n",
    "        if self.environment.ongoing == -1:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        #pass\n",
    "\n",
    "    def legal_actions(self) -> List[Action]:\n",
    "        # Game specific calculation of legal actions.\n",
    "        #New\n",
    "        return self.environment.legal_actions()\n",
    "        #return []\n",
    "\n",
    "    def apply(self, action: Action):\n",
    "        reward = self.environment.step(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.history.append(action)\n",
    "\n",
    "    def store_search_statistics(self, root: Node):\n",
    "        sum_visits = sum(child.visit_count for child in root.children.values())\n",
    "        action_space = (Action(index) for index in range(self.action_space_size))\n",
    "        self.child_visits.append([\n",
    "            root.children[a].visit_count / sum_visits if a in root.children else 0\n",
    "            for a in action_space\n",
    "        ])\n",
    "        self.root_values.append(root.value())\n",
    "\n",
    "    def make_image(self, state_index: int):\n",
    "        # Game specific feature planes.\n",
    "        return []\n",
    "\n",
    "    def make_target(self, state_index: int, num_unroll_steps: int, td_steps: int,\n",
    "                  to_play: Player):\n",
    "    # The value target is the discounted root value of the search tree N steps\n",
    "    # into the future, plus the discounted sum of all rewards until then.\n",
    "        targets = []\n",
    "        for current_index in range(state_index, state_index + num_unroll_steps + 1):\n",
    "            bootstrap_index = current_index + td_steps\n",
    "            if bootstrap_index < len(self.root_values):\n",
    "                value = self.root_values[bootstrap_index] * self.discount**td_steps\n",
    "            else:\n",
    "                value = 0\n",
    "\n",
    "            for i, reward in enumerate(self.rewards[current_index:bootstrap_index]):\n",
    "                value += reward * self.discount**i  # pytype: disable=unsupported-operands\n",
    "\n",
    "            if current_index < len(self.root_values):\n",
    "                targets.append((value, self.rewards[current_index],\n",
    "                            self.child_visits[current_index]))\n",
    "            else:\n",
    "            # States past the end of games are treated as absorbing states.\n",
    "                targets.append((0, 0, []))\n",
    "        return targets\n",
    "\n",
    "    def to_play(self) -> Player:\n",
    "        return Player()\n",
    "\n",
    "    def action_history(self) -> ActionHistory:\n",
    "        return ActionHistory(self.history, self.action_space_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, config: MuZeroConfig):\n",
    "        self.window_size = config.window_size\n",
    "        self.batch_size = config.batch_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def save_game(self, game):\n",
    "        if len(self.buffer) > self.window_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(game)\n",
    "\n",
    "    def sample_batch(self, num_unroll_steps: int, td_steps: int):\n",
    "        games = [self.sample_game() for _ in range(self.batch_size)]\n",
    "        game_pos = [(g, self.sample_position(g)) for g in games]\n",
    "        return [(g.make_image(i), g.history[i:i + num_unroll_steps],\n",
    "                 g.make_target(i, num_unroll_steps, td_steps, g.to_play()))\n",
    "                for (g, i) in game_pos]\n",
    "\n",
    "    def sample_game(self) -> GameHistory:\n",
    "    # Sample game from buffer either uniformly or according to some priority.\n",
    "        return self.buffer[0]\n",
    "\n",
    "    def sample_position(self, game) -> int:\n",
    "    # Sample position from game either uniformly or according to some priority.\n",
    "        return -1\n",
    "\n",
    "\n",
    "class NetworkOutput(typing.NamedTuple):\n",
    "    value: float\n",
    "    reward: float\n",
    "    policy_logits: Dict[Action, float]\n",
    "    hidden_state: List[float]\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def initial_inference(self, image) -> NetworkOutput:\n",
    "    # representation + prediction function\n",
    "        return NetworkOutput(0, 0, {}, [])\n",
    "\n",
    "    def recurrent_inference(self, hidden_state, action) -> NetworkOutput:\n",
    "    # dynamics + prediction function\n",
    "        return NetworkOutput(0, 0, {}, [])\n",
    "\n",
    "    def get_weights(self):\n",
    "    # Returns the weights of this network.\n",
    "        return []\n",
    "\n",
    "    def training_steps(self) -> int:\n",
    "    # How many steps / batches the network has been trained for.\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedStorage(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._networks = {}\n",
    "\n",
    "    def latest_network(self) -> Network:\n",
    "        if self._networks:\n",
    "            return self._networks[max(self._networks.keys())]\n",
    "        else:\n",
    "      # policy -> uniform, value -> 0, reward -> 0\n",
    "            return make_uniform_network()\n",
    "\n",
    "    def save_network(self, step: int, network: Network):\n",
    "        self._networks[step] = network\n",
    "\n",
    "\n",
    "##### End Helpers ########\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MuZero training is split into two independent parts: Network training and\n",
    "# self-play data generation.\n",
    "# These two parts only communicate by transferring the latest network checkpoint\n",
    "# from the training to the self-play, and the finished games from the self-play\n",
    "# to the training.\n",
    "def muzero(config: MuZeroConfig):\n",
    "    storage = SharedStorage()\n",
    "    replay_buffer = ReplayBuffer(config)\n",
    "\n",
    "    for _ in range(config.num_actors):\n",
    "        launch_job(run_selfplay, config, storage, replay_buffer)\n",
    "\n",
    "    train_network(config, storage, replay_buffer)\n",
    "\n",
    "    return storage.latest_network()\n",
    "\n",
    "\n",
    "##################################\n",
    "####### Part 1: Self-Play ########\n",
    "\n",
    "\n",
    "# Each self-play job is independent of all others; it takes the latest network\n",
    "# snapshot, produces a game and makes it available to the training job by\n",
    "# writing it to a shared replay buffer.\n",
    "def run_selfplay(config: MuZeroConfig, storage: SharedStorage,\n",
    "                 replay_buffer: ReplayBuffer):\n",
    "    while True:\n",
    "        network = storage.latest_network()\n",
    "        Game = config.Game\n",
    "        game = play_game(config, network)\n",
    "        replay_buffer.save_game(game)\n",
    "\n",
    "# Each game is produced by starting at the initial board position, then\n",
    "# repeatedly executing a Monte Carlo Tree Search to generate moves until the end\n",
    "# of the game is reached.\n",
    "def play_game(config: MuZeroConfig, network: Network) -> GameHistory:\n",
    "    #Game = config.Game\n",
    "    game = config.new_game()\n",
    "    game_history = GameHistory(config.action_space,config.discount)\n",
    "    \n",
    "    done = False\n",
    "\n",
    "    while not game.terminal() and len(game_history.history) < config.max_moves:\n",
    "# At the root of the search tree we use the representation function to\n",
    "# obtain a hidden state given the current observation.\n",
    "        root = Node(0)\n",
    "        current_observation = game.make_image(-1)\n",
    "        expand_node(root, game.to_play(), game.legal_actions(),\n",
    "                    network.initial_inference(current_observation))\n",
    "        add_exploration_noise(config, root)\n",
    "\n",
    "# We then run a Monte Carlo Tree Search using only action sequences and the\n",
    "# model learned by the network.\n",
    "        run_mcts(config, root, game.action_history(), network)\n",
    "        action = select_action(config, len(game.history), root, network)\n",
    "        game.apply(action)\n",
    "        game_history.store_search_statistics(root)\n",
    "    return game_history\n",
    "\n",
    "\n",
    "    # Core Monte Carlo Tree Search algorithm.\n",
    "    # To decide on an action, we run N simulations, always starting at the root of\n",
    "    # the search tree and traversing the tree according to the UCB formula until we\n",
    "    # reach a leaf node.\n",
    "def run_mcts(config: MuZeroConfig, root: Node, action_history: ActionHistory,\n",
    "         network: Network):\n",
    "    min_max_stats = MinMaxStats(config.known_bounds)\n",
    "\n",
    "    for _ in range(config.num_simulations):\n",
    "        history = action_history.clone()\n",
    "        node = root\n",
    "        search_path = [node]\n",
    "\n",
    "        while node.expanded():\n",
    "            action, node = select_child(config, node, min_max_stats)\n",
    "            history.add_action(action)\n",
    "            search_path.append(node)\n",
    "\n",
    "        # Inside the search tree we use the dynamics function to obtain the next\n",
    "        # hidden state given an action and the previous hidden state.\n",
    "        parent = search_path[-2]\n",
    "        network_output = network.recurrent_inference(parent.hidden_state,\n",
    "                                                     history.last_action())\n",
    "        expand_node(node, history.to_play(), history.action_space(), network_output)\n",
    "\n",
    "        backpropagate(search_path, network_output.value, history.to_play(),\n",
    "                      config.discount, min_max_stats)\n",
    "\n",
    "\n",
    "def select_action(config: MuZeroConfig, num_moves: int, node: Node,\n",
    "              network: Network):\n",
    "    visit_counts = [\n",
    "      (child.visit_count, action) for action, child in node.children.items()\n",
    "    ]\n",
    "    t = config.visit_softmax_temperature_fn(\n",
    "      num_moves=num_moves, training_steps=network.training_steps())\n",
    "    _, action = softmax_sample(visit_counts, t)\n",
    "    return action\n",
    "\n",
    "\n",
    "# Select the child with the highest UCB score.\n",
    "def select_child(config: MuZeroConfig, node: Node,\n",
    "                 min_max_stats: MinMaxStats):\n",
    "    _, action, child = max(\n",
    "      (ucb_score(config, node, child, min_max_stats), action,\n",
    "       child) for action, child in node.children.items())\n",
    "    return action, child\n",
    "\n",
    "\n",
    "# The score for a node is based on its value, plus an exploration bonus based on\n",
    "# the prior.\n",
    "def ucb_score(config: MuZeroConfig, parent: Node, child: Node,\n",
    "          min_max_stats: MinMaxStats) -> float:\n",
    "    pb_c = math.log((parent.visit_count + config.pb_c_base + 1) /\n",
    "                  config.pb_c_base) + config.pb_c_init\n",
    "    pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "\n",
    "    prior_score = pb_c * child.prior\n",
    "    value_score = min_max_stats.normalize(child.value())\n",
    "    return prior_score + value_score\n",
    "\n",
    "\n",
    "# We expand a node using the value, reward and policy prediction obtained from\n",
    "# the neural network.\n",
    "def expand_node(node: Node, to_play: Player, actions: List[Action],\n",
    "                network_output: NetworkOutput):\n",
    "    node.to_play = to_play\n",
    "    node.hidden_state = network_output.hidden_state\n",
    "    node.reward = network_output.reward\n",
    "    policy = {a: math.exp(network_output.policy_logits[a]) for a in actions}\n",
    "    policy_sum = sum(policy.values())\n",
    "    for action, p in policy.items():\n",
    "        node.children[action] = Node(p / policy_sum)\n",
    "\n",
    "\n",
    "    # At the end of a simulation, we propagate the evaluation all the way up the\n",
    "    # tree to the root.\n",
    "def backpropagate(search_path: List[Node], value: float, to_play: Player,\n",
    "              discount: float, min_max_stats: MinMaxStats):\n",
    "    for node in search_path:\n",
    "        node.value_sum += value if node.to_play == to_play else -value\n",
    "        node.visit_count += 1\n",
    "        min_max_stats.update(node.value())\n",
    "\n",
    "        value = node.reward + discount * value\n",
    "\n",
    "\n",
    "    # At the start of each search, we add dirichlet noise to the prior of the root\n",
    "    # to encourage the search to explore new actions.\n",
    "def add_exploration_noise(config: MuZeroConfig, node: Node):\n",
    "    actions = list(node.children.keys())\n",
    "    noise = numpy.random.dirichlet([config.root_dirichlet_alpha] * len(actions))\n",
    "    frac = config.root_exploration_fraction\n",
    "    for a, n in zip(actions, noise):\n",
    "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
    "\n",
    "\n",
    "######### End Self-Play ##########\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##################################\n",
    "####### Part 2: Training #########\n",
    "\n",
    "\n",
    "def train_network(config: MuZeroConfig, storage: SharedStorage,\n",
    "                  replay_buffer: ReplayBuffer):\n",
    "    network = Network()\n",
    "    learning_rate = config.lr_init * config.lr_decay_rate**(\n",
    "        tf.train.get_global_step() / config.lr_decay_steps)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, config.momentum)\n",
    "\n",
    "    for i in range(config.training_steps):\n",
    "        if i % config.checkpoint_interval == 0:\n",
    "            storage.save_network(i, network)\n",
    "        batch = replay_buffer.sample_batch(config.num_unroll_steps, config.td_steps)\n",
    "        update_weights(optimizer, network, batch, config.weight_decay)\n",
    "    storage.save_network(config.training_steps, network)\n",
    "\n",
    "\n",
    "def update_weights(optimizer: tf.compat.v1.train.Optimizer, network: Network, batch,\n",
    "                   weight_decay: float):\n",
    "    loss = 0\n",
    "    for image, actions, targets in batch:\n",
    "        # Initial step, from the real observation.\n",
    "        value, reward, policy_logits, hidden_state = network.initial_inference(\n",
    "            image)\n",
    "        predictions = [(1.0, value, reward, policy_logits)]\n",
    "\n",
    "        # Recurrent steps, from action and previous hidden state.\n",
    "        for action in actions:\n",
    "            value, reward, policy_logits, hidden_state = network.recurrent_inference(\n",
    "              hidden_state, action)\n",
    "            predictions.append((1.0 / len(actions), value, reward, policy_logits))\n",
    "\n",
    "            hidden_state = tf.scale_gradient(hidden_state, 0.5)\n",
    "\n",
    "        for prediction, target in zip(predictions, targets):\n",
    "            gradient_scale, value, reward, policy_logits = prediction\n",
    "            target_value, target_reward, target_policy = target\n",
    "\n",
    "            l = (\n",
    "              scalar_loss(value, target_value) +\n",
    "              scalar_loss(reward, target_reward) +\n",
    "              tf.nn.softmax_cross_entropy_with_logits(\n",
    "                  logits=policy_logits, labels=target_policy))\n",
    "\n",
    "            loss += tf.scale_gradient(l, gradient_scale)\n",
    "\n",
    "    for weights in network.get_weights():\n",
    "        loss += weight_decay * tf.nn.l2_loss(weights)\n",
    "\n",
    "    optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "def scalar_loss(prediction, target) -> float:\n",
    "    # MSE in board games, cross entropy between categorical values in Atari.\n",
    "    return -1\n",
    "\n",
    "######### End Training ###########\n",
    "##################################\n",
    "\n",
    "################################################################################\n",
    "############################# End of pseudocode ################################\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# Stubs to make the typechecker happy.\n",
    "def softmax_sample(distribution, temperature: float):\n",
    "    return 0, 0\n",
    "\n",
    "\n",
    "def launch_job(f, *args):\n",
    "    f(*args)\n",
    "\n",
    "\n",
    "def make_uniform_network():\n",
    "    return Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.MuZeroConfig'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Game',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'action_space_size',\n",
       " 'batch_size',\n",
       " 'checkpoint_interval',\n",
       " 'discount',\n",
       " 'game_name',\n",
       " 'known_bounds',\n",
       " 'lr_decay_rate',\n",
       " 'lr_decay_steps',\n",
       " 'lr_init',\n",
       " 'max_moves',\n",
       " 'momentum',\n",
       " 'new_game',\n",
       " 'num_actors',\n",
       " 'num_simulations',\n",
       " 'num_unroll_steps',\n",
       " 'pb_c_base',\n",
       " 'pb_c_init',\n",
       " 'root_dirichlet_alpha',\n",
       " 'root_exploration_fraction',\n",
       " 'td_steps',\n",
       " 'training_steps',\n",
       " 'visit_softmax_temperature_fn',\n",
       " 'weight_decay',\n",
       " 'window_size']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_config = make_tictactoe_config()\n",
    "\n",
    "print(type(my_config))\n",
    "dir(my_config)\n",
    "#muzero(my_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'game_module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-68fa86c7bf1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmuzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-108-e8ab1de6f219>\u001b[0m in \u001b[0;36mmuzero\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_actors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mlaunch_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_selfplay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-109-1096a8455cf5>\u001b[0m in \u001b[0;36mlaunch_job\u001b[1;34m(f, *args)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlaunch_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-108-e8ab1de6f219>\u001b[0m in \u001b[0;36mrun_selfplay\u001b[1;34m(config, storage, replay_buffer)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mGame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mgame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-108-e8ab1de6f219>\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(config, network)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# of the game is reached.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMuZeroConfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mGameHistory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mgame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mterminal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_moves\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-6323d01cfb80>\u001b[0m in \u001b[0;36mnew_game\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnew_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mGameHistory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-105-1ce3f0bdaf00>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, action_space_size, discount)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m#self.environment = Environment()  # Game specific environment.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m#New\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;31m#End\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'game_module' is not defined"
     ]
    }
   ],
   "source": [
    "muzero(my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dan_tf",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
